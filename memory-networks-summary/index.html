<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222">












<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />






















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=6.0.6" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.0.6">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.0.6">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.0.6">


  <link rel="mask-icon" href="/images/logo.svg?v=6.0.6" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '6.0.6',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="就读过的 Memory Networks 相关论文做一个简单的汇总。 相关网络结构  Memory Networks / Memory Neural Networks（MemNNs） End-to-End Memory Networks（MemN2N） Key-Value Memory Networks（KV-MemNNs） Dynamic Memory Networks（DMN） Imporve">
<meta name="keywords" content="MemNNs,MemN2N,DMN,DMN+,KV-MemNNs,学习笔记,未完成">
<meta property="og:type" content="article">
<meta property="og:title" content="学习笔记——Memory Networks 相关论文总结">
<meta property="og:url" content="http://yoursite.com/memory-networks-summary/index.html">
<meta property="og:site_name" content="ro&#39;s blog">
<meta property="og:description" content="就读过的 Memory Networks 相关论文做一个简单的汇总。 相关网络结构  Memory Networks / Memory Neural Networks（MemNNs） End-to-End Memory Networks（MemN2N） Key-Value Memory Networks（KV-MemNNs） Dynamic Memory Networks（DMN） Imporve">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/memory-networks-summary/1526627841680.png">
<meta property="og:image" content="http://yoursite.com/memory-networks-summary/1526342680974.png">
<meta property="og:image" content="http://yoursite.com/memory-networks-summary/1526434080487.png">
<meta property="og:image" content="http://yoursite.com/memory-networks-summary/1526435763402.png">
<meta property="og:image" content="http://yoursite.com/memory-networks-summary/1526544087243.png">
<meta property="og:image" content="http://yoursite.com/memory-networks-summary/1526544102015.png">
<meta property="og:image" content="http://yoursite.com/memory-networks-summary/1526605717780.png">
<meta property="og:image" content="http://yoursite.com/memory-networks-summary/1526605726799.png">
<meta property="og:updated_time" content="2018-06-05T02:58:49.611Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="学习笔记——Memory Networks 相关论文总结">
<meta name="twitter:description" content="就读过的 Memory Networks 相关论文做一个简单的汇总。 相关网络结构  Memory Networks / Memory Neural Networks（MemNNs） End-to-End Memory Networks（MemN2N） Key-Value Memory Networks（KV-MemNNs） Dynamic Memory Networks（DMN） Imporve">
<meta name="twitter:image" content="http://yoursite.com/memory-networks-summary/1526627841680.png">






  <link rel="canonical" href="http://yoursite.com/memory-networks-summary/"/>



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>学习笔记——Memory Networks 相关论文总结 | ro's blog</title>
  









  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"> 

<div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">ro's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        
          
  <li class="menu-item menu-item-home">
    <a href="/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-home"></i> <br />Home</a>
</li>

      
        
        
          
  <li class="menu-item menu-item-about">
    <a href="/about/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-user"></i> <br />About</a>
</li>

      
        
        
          
  <li class="menu-item menu-item-tags">
    <a href="/tags/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />Tags</a>
</li>

      
        
        
          
  <li class="menu-item menu-item-categories">
    <a href="/categories/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-th"></i> <br />Categories</a>
</li>

      
        
        
          
  <li class="menu-item menu-item-archives">
    <a href="/archives/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />Archives</a>
</li>

      

      
    </ul>
  

  
    

    
    
      
      
    
      
      
    
      
      
    
      
      
    
      
      
    
    

  


  

  
</nav>


  



 </div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/memory-networks-summary/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="rocuku">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ro's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">学习笔记——Memory Networks 相关论文总结</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-04-18T10:50:20+08:00">2018-04-18</time>
            

            
            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/学习笔记/" itemprop="url" rel="index"><span itemprop="name">学习笔记</span></a></span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/memory-networks-summary/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/memory-networks-summary/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>就读过的 Memory Networks 相关论文做一个简单的汇总。</p>
<p><strong>相关网络结构</strong></p>
<ul>
<li>Memory Networks / Memory Neural Networks（MemNNs）</li>
<li>End-to-End Memory Networks（MemN2N）</li>
<li>Key-Value Memory Networks（KV-MemNNs）</li>
<li>Dynamic Memory Networks（DMN）</li>
<li>Imporved Dynamic Memory Networks（DMN+）</li>
</ul>
<a id="more"></a>
<p><strong>相关论文</strong></p>
<ul>
<li>《Memory Networks》[<a href="https://arxiv.org/abs/1410.3916v11" target="_blank" rel="noopener">arxiv</a>，<a href="https://rocuku.github.io/memory-networks/" target="_blank" rel="noopener">笔记</a>]</li>
<li>《Large-scale Simple Question Answering with Memory Networks》[<a href="https://arxiv.org/abs/1506.02075v1" target="_blank" rel="noopener">arxiv</a>，<a href="https://rocuku.github.io/Large-scale-Simple-Question-Answering-with-Memory-Networks/" target="_blank" rel="noopener">笔记</a>]</li>
<li>《End-To-End Memory Networks》[<a href="https://arxiv.org/abs/1503.08895" target="_blank" rel="noopener">arxiv</a>，<a href="https://rocuku.github.io/End-To-End-Memory-Networks/" target="_blank" rel="noopener">笔记</a>]</li>
<li>《Key-Value Memory Networks for Directly Reading Documents》[<a href="https://arxiv.org/abs/1606.03126" target="_blank" rel="noopener">arxiv</a>，<a href="https://rocuku.github.io/Key-Value-Memory-Networks-for-Directly-Reading-Documents/" target="_blank" rel="noopener">笔记</a>]</li>
<li>《Ask Me Anything: Dynamic Memory Networks for Natural Language Processing》[<a href="https://arxiv.org/abs/1506.07285" target="_blank" rel="noopener">arxiv</a>，<a href="https://rocuku.github.io/Ask-Me-Anything-Dynamic-Memory-Networks-for-Natural-Language-Processing/" target="_blank" rel="noopener">笔记</a>]</li>
<li>《Dynamic Memory Networks for Visual and Textual Question Answering》[<a href="https://arxiv.org/abs/1603.01417" target="_blank" rel="noopener">arxiv</a>，<a href="https://rocuku.github.io/Dynamic-Memory-Networks-for-Visual-and-Textual-Question-Answering/" target="_blank" rel="noopener">笔记</a>]</li>
</ul>
<p><strong>相关数据集</strong></p>
<ul>
<li><a href="https://research.fb.com/downloads/babi/" target="_blank" rel="noopener">bAbI</a>
<ul>
<li>bAbI-tasks（DBQA）</li>
<li>WikiMovies（MovieQA）（DBQA + KBQA）</li>
<li>SimpleQuestions（KBQA）</li>
</ul></li>
<li><a href="https://www.microsoft.com/en-us/download/details.aspx?id=52763" target="_blank" rel="noopener">WebQuestions</a>（KBQA）</li>
<li><a href="https://www.microsoft.com/en-us/download/details.aspx?id=52419" target="_blank" rel="noopener">WikiQA</a>（DBQA）</li>
</ul>
<p><strong>相关 KB</strong></p>
<ul>
<li>Freebase</li>
<li>Reverb</li>
</ul>
<h2 id="memory-networks">Memory Networks</h2>
<p>由《Memory Networks》[<a href="https://arxiv.org/abs/1410.3916v11" target="_blank" rel="noopener">arxiv</a>，<a href="https://rocuku.github.io/memory-networks/" target="_blank" rel="noopener">笔记</a>] 提出的一个用于解决 long-term memory 问题的框架。</p>
<h3 id="基本框架">基本框架</h3>
<p>Memory Networks 基本框架包括 1 个 memory 和 4 个组件：</p>
<ul>
<li><strong>I（input feature map）</strong><br>
把输入映射为内部特征表示（internel feature representation）<span class="math inline">\(I(x)\)</span></li>
<li><strong>G（generalization）</strong><br>
使用新的输入更新旧的 memories（相当于把 <strong>I 模块</strong>的输出存入 memory）：<span class="math inline">\(m_i=G(m_i,I(x),m),\forall i\)</span></li>
<li><strong>O（output feature map）</strong><br>
通过新的输入和当前 memory 状态，在特征空间里产生新的输出：<span class="math inline">\(o=O(I(x),m)\)</span></li>
<li><strong>R（response）</strong><br>
将特征空间输出解码为任务要求格式：<span class="math inline">\(r=R(o)\)</span></li>
</ul>
<p>要注意的是，测试的时候一样会更新 memory，只是不再更新各个组件的参数。 这些组件可以使用任意现存实现方式（SVM，决策树什么的都可以）</p>
<h3 id="memnns">MemNNs</h3>
<p><strong>MemNNs，即 memory neural networks，组件使用 NN 实现的 memory networks</strong>（这才是 MemNNs 的正式定义，之前我一直以为 memory network 就是 MemNNs……）</p>
<p>同样由《Memory Networks》[<a href="https://arxiv.org/abs/1410.3916v11" target="_blank" rel="noopener">arxiv</a>，<a href="https://rocuku.github.io/memory-networks/" target="_blank" rel="noopener">笔记</a>] 提出的一个针对文本形式输入输出的 MemNNs 实施例</p>
<h4 id="基本模型">基本模型</h4>
<ul>
<li><strong>I 模块</strong>
<ul>
<li>输入 text，直接原格式输出</li>
<li>在注解里提到，技术上来说，会使用 embedding 模型获得 text 对应的 embedding 向量作为输出，而不是直接输出（我觉得正文里之所以提到直接使用 original form 是为了举一个最简单的模型示例）</li>
<li>使用 embedding 向量的优缺点：在训练过程中 embedding 矩阵的参数在变，因此已经存入 memory 里的 embedding 向量会过时；但是在测试过程中，因为 embedding 矩阵的参数不会改变，所以 memory 不会过时，相比于直接存储原始文字，存入 embedding 向量使得每次使用 memory 的时候不用再进行重复的 embedding 操作，提高了效率</li>
</ul></li>
<li><strong>G 模块</strong>
<ul>
<li>用于将 <strong>I 模块</strong> 的输出存入下一个可用的 memory slot 里</li>
<li><span class="math inline">\(S(x)\)</span> 返回下一个为空的 memory slot <span class="math inline">\(N\)</span>：<span class="math inline">\(m_N=x\)</span>，<span class="math inline">\(N=N+1\)</span></li>
</ul></li>
<li><strong>O 模块</strong>
<ul>
<li>模块找出 <span class="math inline">\(k\)</span> 个针对输入 <span class="math inline">\(x\)</span> 的 supporting memories 以生产输出 <span class="math inline">\(o\)</span></li>
<li>假设 <span class="math inline">\(k\)</span> 最大到 2（实际生产里这个 <span class="math inline">\(k\)</span> 会更大）
<ul>
<li><span class="math inline">\(k=1\)</span>，先选出得分最高的 supporting memory（最相关）：<span class="math inline">\(o_1=O_1(x, m) = \mathop{\arg\max}\limits_{i=1,...,N}s_O(x,m_i)\)</span></li>
<li><span class="math inline">\(k=2\)</span>，根据 <span class="math inline">\(o_1\)</span> 选出排第二的 supporting memory：<span class="math inline">\(o_2=O_2(x,m)=\mathop{\arg\max}\limits_{i=1,...,N}s_O([x,m_{o_1}],m_i)\)</span></li>
<li>最终输出：<span class="math inline">\(o=[x, m_{o_1},m_{o_2}]\)</span></li>
</ul></li>
</ul></li>
<li><strong>R模块</strong>
<ul>
<li>根据 <strong>O 模块</strong> 的输出输出文本响应：<span class="math inline">\(r=\arg\max_{w\in W}s_R([x,m_{o_1}m_{o_2}],w)\)</span></li>
</ul></li>
</ul>
<p>其中，<span class="math inline">\(s_O\)</span> 和 <span class="math inline">\(s_R\)</span> 都是 score 函数：<span class="math inline">\(s(x,y)=\Phi_x(x)U^\top U^\top \Phi_y(y)\)</span><br>
<span class="math inline">\(U\)</span> 是一个 <span class="math inline">\(n\times D\)</span> 矩阵，<span class="math inline">\(D\)</span> 是特征数量，<span class="math inline">\(n\)</span> 是 embedding 维度。<span class="math inline">\(\Phi_x\)</span> 和 <span class="math inline">\(\Phi_y\)</span> 是用于将原始 text 映射到 <span class="math inline">\(D\)</span> 维特征空间里的。<br>
最简单的特征空间就是 bag of words（BOW），另 <span class="math inline">\(D=3|W|\)</span>，即每个词在字典里有 3 个不同的表示：1 个针对 <span class="math inline">\(\Phi_y(\cdot)\)</span>，2 个针对 <span class="math inline">\(\Phi_x(\cdot)\)</span>，依据这个输入来自 <span class="math inline">\(x\)</span> 还是 supporting memories。<span class="math inline">\(s_R\)</span> 和 <span class="math inline">\(S_O\)</span> 使用不同的权重矩阵 <span class="math inline">\(U_R\)</span>、<span class="math inline">\(U_O\)</span>（这段还是不太懂）</p>
<h4 id="训练">训练</h4>
<p>强监督训练，训练集里需要 inputs、responses 以及经过 labeled 的 supporting sentences，即训练过程中能够知道 <span class="math inline">\(o_1,o_2\)</span> 的正确值。（然而这种值在 RNN、LSTM 的训练中并不好应用）</p>
<p>训练使用 margin ranking loss（ 即要求正确答案的得分比错误答案的得分高至少一个margin <span class="math inline">\(\gamma\)</span>），使用随机梯度下降（SGD）</p>
<p>给出 question <span class="math inline">\(x\)</span>，response <span class="math inline">\(r\)</span>，supporting sentences <span class="math inline">\(m_{o_1}\)</span>，<span class="math inline">\(m_{o_2}\)</span>，训练过程最小化下述式子（<code>这公式后两行的方括号什么情况…</code>） <span class="math display">\[
\sum_{\bar f \not=m_{o_2}}\max(0,\gamma-s_O(x,m_{o_1})+s_O(x,\bar f))+\\
\sum_{\bar {f&#39;}\not=m_{o_2}}\max(0,\gamma-s_O([x,m_{o_1}],m_{o_2}])+s_O([x,m_{o_1}],\bar{f&#39;}]))+\\
\sum_{\bar r\not=r}\max(0,\gamma-s_R([x,m_{o_1},m_{o_2}],r)+s_R([x,m_{o_1},m_{o_2}],\bar r]))
\]</span> 其中，<span class="math inline">\(r\)</span> 是正确答案， <span class="math inline">\(\bar f\)</span>，<span class="math inline">\(\bar{f&#39;}\)</span> 和 <span class="math inline">\(\bar r\)</span> 是所有的错误标签（SGD 里不是所有的，而是随机采样出来的）</p>
<ul>
<li>第一行代表有没有正确挑选出得分最高的 supporting memory <span class="math inline">\(m_{o_1}\)</span></li>
<li>第二行代表正确挑选出了 <span class="math inline">\(m_{o_1}\)</span> 后能不能正确找出 <span class="math inline">\(m_{o_2}\)</span></li>
<li>第三行代表把正确的 supporting fact 作为输入，能不能挑选出正确的答案输入 <strong>R 模块</strong></li>
</ul>
<h4 id="一些扩展">一些扩展</h4>
<ul>
<li><strong>使用词序列作为输入</strong><br>
添加 embedding 模型 segmenter：<span class="math inline">\(seg(c) = W_{seg}^\top U_S\Phi_{seg}(c)\)</span></li>
<li><strong>使用哈希的高效 memory</strong></li>
<li><strong>Extend model to take into account when a memory slot was written to</strong>（没太看懂）</li>
<li><strong>用语言模型处理未知词</strong></li>
<li><strong>add the “bag of words” matching score to the learned embedding score</strong>（没太看懂）</li>
</ul>
<h3 id="针对-large-scale-simple-qa-的-memnns">针对 Large-scale Simple QA 的 MemNNs</h3>
<p>由《Large-scale Simple Question Answering with Memory Networks》[<a href="https://arxiv.org/abs/1506.02075v1" target="_blank" rel="noopener">arxiv</a>，<a href="https://rocuku.github.io/Large-scale-Simple-Question-Answering-with-Memory-Networks/" target="_blank" rel="noopener">笔记</a>] 提出的针对大规模 simple QA 的MemNNs。</p>
<p>这个是 MemNNs 在 KBQA 上的应用，这篇文章的模型到现在应用价值可能不是很高，但是它提出的数据集 SimpleQuestions 是现在 KBQA 问题的最大数据集了，并且只包含 Simple QA 问题（这个是优点也是缺点吧）。另外它对 KB 的预处理方式也值得参考。</p>
<h4 id="基本模型-1">基本模型</h4>
<ul>
<li><strong>I 模块</strong>
<ul>
<li>对 Freebase facts、questions、Reverb facts 进行预处理，将其转化为向量表示。</li>
<li><strong>预处理Freebase：</strong>把 Freebase 的数据处理成适配 QA 任务的形式
<ul>
<li><strong>grouping：</strong>把具有相同 subject 和 relation 的 fact 聚合成一个新的 fact，应对多答案问题。例如 <span class="math inline">\((s, r, o_1)\)</span> 和 <span class="math inline">\((s, r, o_2)\)</span> 合成为 <span class="math inline">\((s, r, \{o_1, o_2\})\)</span></li>
<li><strong>删除中间节点：</strong>把多个 fact 表示的复杂关系简化为1个 fact，这样减少推理过程，让 simple QA 可以解决更多问题。例如 <span class="math inline">\((s, r_1, o_1)\)</span> 和 <span class="math inline">\((o_1, r_2, o_2)\)</span> 合成为 <span class="math inline">\((s, r_2, o_2)\)</span></li>
</ul></li>
<li><strong>预处理Freebase facts：</strong>向量化三元组，使用 bag-of-symbol 表征， 得到 <span class="math inline">\(N_S\)</span> 维（entities 和 relationships 数量之和）的向量 <span class="math inline">\(f(y)\)</span>。<span class="math inline">\((s, r, \{o_1, o_2, ..., o_k\})\)</span> 这类聚合三元组里每个实体的代表维度的值为 <span class="math inline">\(1/k\)</span> 而不是 <span class="math inline">\(1\)</span></li>
<li><strong>预处理questions：</strong>向量化问题，使用 bag-of-ngrams 表征，得到 <span class="math inline">\(N_V\)</span> 维（字典大小）的向量 <span class="math inline">\(g(q)\)</span></li>
<li><strong>预处理Reverb facts：</strong>向量化三元组，使用 bag-of-symbol 表征 s 和 o，使用 bag-of-words 表征 r，得到 <span class="math inline">\(N_S+N_V\)</span> 维的向量 <span class="math inline">\(h(y)\)</span></li>
</ul></li>
<li><strong>G 模块</strong>
<ul>
<li>用于向 memory 中添加新元素，就是将 Reverb facts 和 Freebase-based memory structure 建立连接（用于迁移学习）</li>
</ul></li>
<li><strong>O 模块</strong>
<ul>
<li>根据输入，找到对应的 supporting fact</li>
<li><strong>生成候选facts：</strong>通过问题的 n-grams of words 在 Freebase 里找到少量相关匹配实体</li>
<li><strong>Scoring：</strong>使用 embedding 模型对候选 facts 进行评分，最高得分的为 supporting fact
<ul>
<li>question <span class="math inline">\(q\)</span> 和 Freebase fact <span class="math inline">\(y\)</span> 之间相似度的计算公式：<span class="math inline">\(S_{QA}(q,y)=cos(W_Vg(q),W_Sf(y))\)</span>，<span class="math inline">\(W_V\)</span> 和 <span class="math inline">\(W_S\)</span> 是 embedding 矩阵，维度分别为 <span class="math inline">\(d\times N_V\)</span> 和 <span class="math inline">\(d\times N_S\)</span></li>
<li>question <span class="math inline">\(q\)</span> 和 Reverb fact <span class="math inline">\(y\)</span> 之间相似度的计算公式：<span class="math inline">\(S_{RVB}(q,y)=cos(W_Vg(q),W_{VS}h(y))\)</span>，<span class="math inline">\(W_{VS}\)</span> 也是 embedding 矩阵，维度为 <span class="math inline">\(d\times (N_V+N_S)\)</span></li>
<li><span class="math inline">\(d\)</span> 是超参，embedding 矩阵 <span class="math inline">\(W_V\)</span> 和 <span class="math inline">\(W_S\)</span> 是要通过训练学习的参数</li>
</ul></li>
</ul></li>
<li><strong>R 模块</strong>
<ul>
<li>直接返回 output module 选择的 supporting fact 集合</li>
</ul></li>
</ul>
<h2 id="memn2n">MemN2N</h2>
<p>由《End-To-End Memory Networks》[<a href="https://arxiv.org/abs/1503.08895" target="_blank" rel="noopener">arxiv</a>，<a href="https://rocuku.github.io/End-To-End-Memory-Networks/" target="_blank" rel="noopener">笔记</a>] 提出，解决了 MemNNs 存在的问题：网络每一层都需要强监督，导致训练中难以进行反向传播的计算</p>
<p>本文是以 DBQA 问题为例进行的。</p>
<p>MemN2N 和 MemNNs 的最大不同就是 <strong>O 模块</strong> 的输出 <span class="math inline">\(o\)</span> 计算由取最大值变为了基于 Softmax 的加权平均值。</p>
<h3 id="基本模型-2">基本模型</h3>
<p><strong>架构图</strong> <img src="/memory-networks-summary/1526627841680.png" alt="Alt text"> (a) 为单层模型，(b) 为多层模型</p>
<p>给定输入 <span class="math inline">\(x_1,...,x_n\)</span>，查询（query）<span class="math inline">\(q\)</span>，模型输出回答 <span class="math inline">\(a\)</span></p>
<h4 id="单层模型">单层模型</h4>
<p>参考上图 (a)，模型里有两个 memory 块，一个 input 用于计算每个 memory 的概率 <span class="math inline">\(p_i\)</span>，一个 output 用于根据 <span class="math inline">\(p_i\)</span> 求加权和 <span class="math inline">\(o\)</span> ，这个思路就是 soft attention 嘛</p>
<ul>
<li><strong>I 模块 &amp; G 模块（Input memory representation）</strong>
<ul>
<li>给定输入 <span class="math inline">\(\{x_i\}\)</span>，首先将其 embed 为 <span class="math inline">\(d\)</span> 维的 memory 向量 <span class="math inline">\(\{m_i\}\)</span>（由 embedding 矩阵 <span class="math inline">\(A\)</span> 获得），并存入 memory</li>
<li>给定的查询 <span class="math inline">\(q\)</span> 也 embed 为 <span class="math inline">\(d\)</span> 维获得内部状态（internal state）<span class="math inline">\(u\)</span>（由 embedding 矩阵 <span class="math inline">\(B\)</span> 获得）</li>
<li>在 embedding 空间里计算 <span class="math inline">\(u\)</span> 和 <span class="math inline">\(m_i\)</span> 的相似度（使用softmax计算内积）：<span class="math inline">\(p_i=Softmax(u^Tm_i)\)</span>（其实这一步应该算到 <strong>O 模块</strong> 里）</li>
</ul></li>
<li><strong>O 模块（Output memory representation）</strong>
<ul>
<li>每个 <span class="math inline">\(x_i\)</span> 有一个相关输出向量 <span class="math inline">\(c_i\)</span>（由 embedding 矩阵 <span class="math inline">\(C\)</span> 获得）</li>
<li>response 向量 <span class="math inline">\(o = \sum\limits_ip_ic_i\)</span></li>
</ul></li>
<li><strong>R 模块（Generating the final prediction）</strong>
<ul>
<li>预测标签 <span class="math inline">\(\hat a=Softmax(W(o+u))\)</span>，<span class="math inline">\(W\)</span> 为训练参数</li>
</ul></li>
</ul>
<h4 id="多层模型">多层模型</h4>
<p>参考上图 (b)，多层模型就是单层模型的堆叠。</p>
<p>每一层都有各自的 2 个 memory 块，输入 <span class="math inline">\({x_i}\)</span> 相同，不过每层的 embedding 矩阵 <span class="math inline">\(A\)</span>、<span class="math inline">\(C\)</span> 是独立的；单层模型通过 <span class="math inline">\(u\)</span> 集联，每层输入的 <span class="math inline">\(u\)</span> 都通过上一层的 <span class="math inline">\(u\)</span> 及输出 <span class="math inline">\(o\)</span> 相加获得。</p>
<p>具体第 <span class="math inline">\(k\)</span> 层：</p>
<ul>
<li>当前层的 <span class="math inline">\(m_i\)</span>，<span class="math inline">\(c_i\)</span> 都由 embedding 矩阵 <span class="math inline">\(A^k\)</span>，<span class="math inline">\(C^k\)</span> 获得</li>
<li>下一层的输入 <span class="math inline">\(u^{k+1}=u^k+o^k\)</span></li>
<li>网络最终输出 <span class="math inline">\(\hat a=Softmax(Wu^{K+1})=Softmax(W(o^K+u^K))\)</span></li>
</ul>
<p>两种权重取值方法：</p>
<ol type="1">
<li><strong>Adjacent</strong>
<ul>
<li>每一层的输出 embedding 矩阵是下一层的输入 embedding 矩阵（<span class="math inline">\(A^{k+1}=C^k\)</span>）</li>
<li>预测矩阵和输出 embedding 矩阵相同（<span class="math inline">\(W^T=C^K\)</span>）</li>
<li>question 的 embedding 矩阵和第一层的输入 embedding 矩阵相同（<span class="math inline">\(B=A^1\)</span>）</li>
</ul></li>
<li><strong>Layer-wise（RNN-like）</strong>
<ul>
<li>输入输出 embedding 矩阵每层均相同（<span class="math inline">\(A^1=A^2=...=A^K, C^1=C^2=...=C^K\)</span>）</li>
<li>应用线性映射 <span class="math inline">\(H\)</span> 获得 <span class="math inline">\(u^{k+1}\)</span>：<span class="math inline">\(u^{k+1} = Hu^k+o^k\)</span></li>
<li>这种取值方法下的 MemN2N 和传统 RNN 类似，就是把 RNN 的输出划分为了 internal 和 external 部分（internal 部分考虑 memory，external 部分考虑预测标签）， <span class="math inline">\(u\)</span> 为隐藏状态，<span class="math inline">\(p\)</span> 为 internal 输出。明确规定了在 <span class="math inline">\(K\)</span> 跳过程中保存 outputs 到 memory 里（soft），而不是像 RNN 那样对它们进行采样（sampling）。
<ul>
<li>RNN： <span class="math display">\[\begin{align*}
&amp;o^k = Vu^k  + c  \\
&amp;u^{k+1} = tanh(Ux^k + Wu^{k} + b)  \\
\end{align*}\]</span></li>
<li>MemN2N： <span class="math display">\[\begin{align*}
&amp;p^k=Softmax((u^k)^Tm^k) \\
&amp;u^{k+1}=Hu^k+Cp^k\\
\end{align*}\]</span></li>
</ul></li>
</ul></li>
</ol>
<h2 id="kv-memnns">KV-MemNNs</h2>
<p>由 《Key-Value Memory Networks for Directly Reading Documents》[<a href="https://arxiv.org/abs/1606.03126" target="_blank" rel="noopener">arxiv</a>，<a href="https://rocuku.github.io/Key-Value-Memory-Networks-for-Directly-Reading-Documents/" target="_blank" rel="noopener">笔记</a>] 提出，基于 MemN2N 的架构，将其 memory 存储的形式变为向量对 <span class="math inline">\((k_1,v_1),...,(k_M,v_M)\)</span></p>
<p>本文提出了直接使用文档数据替代 KB，解决 KBQA 问题。另外为了验证 KV-MemNNs 在文档和 KB 上的效果，构造了数据集 WikiMovies。</p>
<h3 id="基本模型-3">基本模型</h3>
<p><strong>架构图</strong> <img src="/memory-networks-summary/1526342680974.png" alt="Alt text"></p>
<p><strong>模块 O</strong> 针对 memory 形式的改变进行了修改</p>
<ol type="1">
<li><strong>Key Hashing：</strong>根据问题 <span class="math inline">\(x\)</span> 预先选择出 N 个memory 对 <span class="math inline">\((k_{h_1},v_{h_1}),...,(k_{h_N},v_{h_N})\)</span>，要求这些 key 至少有一个词和问题里的相同（要求是词频小于 1000 的词，为了排除停用词）</li>
<li><strong>Key Addressing：</strong>通过比较问题和每个 key 计算 memory 对和问题的相关可能性（relevance probability）：<span class="math inline">\(p_{h_i}=Softmax(A\Phi_X(x)\cdot A\Phi_K(k_{h_i}))\)</span>，其中，<span class="math inline">\(\Phi\)</span> 是维度为 D 的 feature maps，<span class="math inline">\(A\)</span> 是 <span class="math inline">\(d\times D\)</span> 矩阵</li>
<li><strong>Value Reading：</strong>使用加权和读取 value，返回 <span class="math inline">\(o=\sum\limits_ip_{h_i}A\Phi_V(v_{h_i})\)</span></li>
</ol>
<p>上面的步骤重复进行 H hops</p>
<ul>
<li><strong>controller</strong> 神经网络使用 <span class="math inline">\(q=A\Phi_X(x)\)</span> 作为 query</li>
<li>在得到 hop 1 的 <span class="math inline">\(o\)</span> 后，hop 2 的 query 更新为 <span class="math inline">\(q_2=R_1(q+o)\)</span>，<span class="math inline">\(R\)</span> 是 <span class="math inline">\(d\times d\)</span> 矩阵。</li>
<li>hop <span class="math inline">\(j\)</span> 有 <span class="math inline">\(R_j\)</span>，<span class="math inline">\(q_{j+1}=R_j(q+o)\)</span>，<span class="math inline">\(p_{h_i}=Softmax(q_{j+1}^\top A\Phi_K(k_{h_i}))\)</span></li>
<li>在 H hop 之后，输出预测结果 <span class="math inline">\(\hat a=\arg\max_{i=1,...,C}Softmax(q_{H+1}^\top B\Phi_Y(y_i))\)</span>，<span class="math inline">\(y_i\)</span> 是可能的候选输出</li>
</ul>
<p>矩阵 <span class="math inline">\(A\)</span>，<span class="math inline">\(B\)</span>，<span class="math inline">\(R_1,...,R_H\)</span> 是训练中需要学习的参数</p>
<h3 id="key-value-实现方式">Key-Value 实现方式</h3>
<ul>
<li><strong>KB Triple</strong><br>
KB，针对 triples <code>(subject, relation, object)</code>，key 是 subject 和 relation，value 是 object。然后需要将 subject 和 object 交换，relation 取反再存一次（KB 大小翻倍）</li>
<li><strong>Sentence Level</strong><br>
document，每个 memory slot 编码一个句子的情况，key 和 value 都是用 bag-of-words 的形式编码的整个句子（和标准 MemNN 里一样）</li>
<li><strong>Window Level</strong><br>
document，分割成 W 个词的窗口（中心词是实体），key 是用 bag-of-words 编码后的整个窗口，value 是中心词</li>
<li><strong>Window + Center Encoding</strong><br>
document，和 <strong>Window Level</strong> 的区别就是使用不同 feature 对中心词和窗口中除了中心词以外的词进行编码</li>
<li><strong>Window + Title</strong><br>
document，key 还是上面两条提到的窗口，value 是 document 的 title，另外也保留 <strong>Window Level</strong> 里的所有 memory 对</li>
</ul>
<h2 id="dmn">DMN</h2>
<p>由《Ask Me Anything: Dynamic Memory Networks for Natural Language Processing》[<a href="https://arxiv.org/abs/1506.07285" target="_blank" rel="noopener">arxiv</a>，<a href="https://rocuku.github.io/Ask-Me-Anything-Dynamic-Memory-Networks-for-Natural-Language-Processing/" target="_blank" rel="noopener">笔记</a>] 提出，这篇文章不是 facebook 的。</p>
<p>本文里提出大部分 NLP 问题都可以转换为 QA 问题，DMN 则是针对 QA 任务设计，因此可以用于几乎所有自然语言任务。</p>
<p>DMN 正式的提出来 Memory Networks 和 Attention 机制的结合使用，前面 MemN2N 之类的都只能说是用到了和 soft attention 类似思路的加权平均值，文章里好像没有正式提到 Attention</p>
<p>DMN 的结构不是 MemNNs 的 <strong>I、G、O、R 模块</strong> 了。</p>
<p>QA 问题的核心：<strong>input-question-answer</strong></p>
<h3 id="模型">模型</h3>
<p><strong>DMN 架构图</strong> <img src="/memory-networks-summary/1526434080487.png" alt="Alt text"></p>
<ul>
<li><strong>Input Module：</strong>编码原始文本数据 input，将其转换成向量表示</li>
<li><strong>Question Module：</strong>编码原始文本数据 question，将其转换成向量表示</li>
<li><strong>Episodic Memory Module：</strong>事件记忆模块，通过 attention 机制选择需要注意的 input 并根据 question 给出 memory 向量。这个模块有 retrieve 新信息的能力</li>
<li><strong>Answer Module：</strong>根据最终的 memory 向量生成答案</li>
</ul>
<p><strong>架构示例</strong> <img src="/memory-networks-summary/1526435763402.png" alt="Alt text"></p>
<h4 id="input-module">Input module</h4>
<p>使用 RNN 对 input 序列进行编码。本文里使用的 GRU</p>
<ul>
<li>模块输入是具有 <span class="math inline">\(T_I\)</span> 个词 <span class="math inline">\(w_1,...,w_{T_I}\)</span> 的序列</li>
<li>每个 step <span class="math inline">\(t\)</span> 里，网络更新隐藏状态 <span class="math inline">\(h_t=RNN(L[w_t],h_{t-1})\)</span>，<span class="math inline">\(L\)</span> 是 embedding 矩阵。</li>
<li>模块的输出是 RNN 的隐藏状态。当输入是句子列表的时候，把数据连接成一个单词的长列表，每个句子结尾加一个 end-of-sentence token。每个 end-of-sentence token 处的隐藏状态就是模块的最终表征。（具体架构示例图）</li>
</ul>
<h4 id="question-module">Question Module</h4>
<p>使用 RNN 对 question 进行编码。</p>
<ul>
<li>模块输入是具有 <span class="math inline">\(T_Q\)</span> 个词的 question</li>
<li>隐藏状态 <span class="math inline">\(q_t=GRU(L[w_t^Q],q_{t-1})\)</span>，Input module 和 question module 共享同一个 embedding 矩阵 <span class="math inline">\(L\)</span></li>
<li>输出是最终的隐藏状态 <span class="math inline">\(q=q_{T_Q}\)</span></li>
</ul>
<h4 id="episodic-memory-module">Episodic Memory Module</h4>
<ul>
<li>事件记忆模块根据 input module 的输出进行迭代，更新模块内部的事件记忆</li>
<li>每次迭代中，attention 机制根据 fact 表征 <span class="math inline">\(c\)</span>，question 表征 <span class="math inline">\(q\)</span> 和前一个 memory <span class="math inline">\(m^{i-1}\)</span> 来生成事件 <span class="math inline">\(e^i\)</span></li>
<li>然后事件和前一个 memory 都用来更新当前 memory：<span class="math inline">\(m^i=GRU(e^i,m^{i-1})\)</span>，GRU 初始化 <span class="math inline">\(m^0=q\)</span></li>
<li><p><span class="math inline">\(T_M\)</span> 之后，模块最终输出 <span class="math inline">\(m^{T_M}\)</span> 到 answer module</p></li>
<li><strong>Need for Multiple Episodes</strong>
<ul>
<li>每次迭代关注不同的输入</li>
<li>例如架构示例里第一次关注 football，第二次关注 John</li>
</ul></li>
<li><strong>Attention Mechanism</strong>
<ul>
<li>gating function：<span class="math inline">\(g_t^i=G(c_t,m^{i-1},q)\)</span>，<span class="math inline">\(c_t\)</span> 为候选 fact，<span class="math inline">\(G\)</span> 是 scoring function</li>
<li>特征向量 <span class="math inline">\(z(c,m,q)=[c,m,q,c\cdot q,c\cdot m,|c-q|,|c-m|,c^TW^{(b)}q,c^TW^{(b)}m]\)</span></li>
<li>scoring function <span class="math inline">\(G(c,m,q)=\sigma(W^{(2)}\tanh(W^{(1)}z(c,m,q)+b^{(1)})+b^{(2)})\)</span></li>
</ul></li>
<li><strong>Memory Update</strong>
<ul>
<li>隐藏状态 <span class="math inline">\(h_t^i=g_t^iGRU(c_t,h_{t-1}^i)+(1-g_t^i)h_{t-1}^i\)</span></li>
<li>事件 <span class="math inline">\(e^i=h_{T_C}^i\)</span></li>
</ul></li>
<li><strong>Criteria for Stopping</strong>
<ul>
<li>如果 gate function 选择到输入中一个特殊的 end-of-passes 表征，则停止迭代</li>
</ul></li>
</ul>
<h4 id="answer-module">Answer Module</h4>
<p>根据任务类型不同，模块可能是最后输出一个 answer，也可能每个 time step 都输出</p>
<ul>
<li>初始化 last memory <span class="math inline">\(a_0=m^{T_M}\)</span>，每个 time step，输入 question <span class="math inline">\(q\)</span>，最后一个隐藏状态 <span class="math inline">\(a_{t-1}\)</span></li>
<li><span class="math inline">\(a_t=GRU([y_{t-1},q],a_{t-1})\)</span></li>
<li>模块输出 <span class="math inline">\(y_t=softmax(W^{(a)}a_t)\)</span></li>
</ul>
<h2 id="dmn-1">DMN+</h2>
<p>由《Dynamic Memory Networks for Visual and Textual Question Answering》[<a href="https://arxiv.org/abs/1603.01417" target="_blank" rel="noopener">arxiv</a>，<a href="https://rocuku.github.io/Dynamic-Memory-Networks-for-Visual-and-Textual-Question-Answering/" target="_blank" rel="noopener">笔记</a>] 提出，作为 DMN 的改进，改进了 DMN 在以下两种情况下的表现：</p>
<ul>
<li>训练时无 supporting facts</li>
<li>使用诸如图片等其他形式的数据（非文本数据）</li>
</ul>
<h3 id="模型-1">模型</h3>
<p>DMN+ 相比于 DMN 修改的两部分：</p>
<ul>
<li>input representation</li>
<li>attention 机制与 memory 更新</li>
</ul>
<h4 id="针对-text-qa-的-input-module">针对 Text QA 的 input module</h4>
<p>在没有 supporting facts 的情况下 DMN 表现不好的原因：</p>
<ul>
<li>GRU 只能让句子从上文中获得信息，而不能从下文中获得</li>
<li>可能一些 supporting 句子相距太远了，对于 word level GRU 无法获得它们间的关联</li>
</ul>
<p><strong>模块示例图</strong> <img src="/memory-networks-summary/1526544087243.png" alt="Alt text"></p>
<p>DMN+ 将 DMN 的 input module 由单向 GRU 换成两个组件，<strong>Sentence reader</strong> 和 <strong>Input fusion layer</strong></p>
<ul>
<li><strong>Sentence reader</strong>
<ul>
<li>负责将输入的词编码为 sentence embedding</li>
<li>输入 word tokens <span class="math inline">\([w_1^i,...,w_{M_i}^i]\)</span>，输出 sentence encoding <span class="math inline">\(f_i\)</span>，其中 <span class="math inline">\(M_i\)</span> 是句子的长度</li>
<li>可以使用任意的编码方式，上图里用的 Positional Encoder，用 GRU 或者 LSTM 也可以</li>
<li><strong>Positional Encoder：</strong>
<ul>
<li><span class="math inline">\(f_i=\sum_M^{j=1}l_j\cdot w_j^i\)</span></li>
<li><span class="math inline">\(l_{jd}=(1-j/m)-(d/D)(1-2j/M)\)</span></li>
<li>其中，<span class="math inline">\(d\)</span> 为 embedding index，<span class="math inline">\(D\)</span> 是 embedding 维度</li>
</ul></li>
</ul></li>
<li><strong>Input fusion layer</strong>
<ul>
<li>负责获得句子间的关联</li>
<li>将 DMN 的单向 GRU 换成双向 GRU（bi-directional GRU）</li>
<li><strong>Bi-GRU：</strong>
<ul>
<li><span class="math inline">\(\overrightarrow{f_i}=GRU_{fwd}(f_i,\overrightarrow{f_{i-1}})\)</span></li>
<li><span class="math inline">\(\overleftarrow{f_i}=GRU_{bwd}(f_i,\overleftarrow{f_{i+1}})\)</span></li>
<li><span class="math inline">\(\overleftrightarrow{f_i}=\overrightarrow{f_i}+\overleftarrow{f_i}\)</span></li>
</ul></li>
</ul></li>
</ul>
<h4 id="针对-vqa-的-input-module">针对 VQA 的 input module</h4>
<p><strong>模块示例图</strong> <img src="/memory-networks-summary/1526544102015.png" alt="Alt text"></p>
<p>DMN+ 针对 VQA 的 input module 分为 3 个 组件</p>
<ul>
<li><strong>Local region feature extraction</strong>
<ul>
<li>使用基于 VGG-19 的 CNN</li>
<li>首先把输入 rescale 到 448x448，最后的池化层输出维度 <span class="math inline">\(d=512\times 14\times 14\)</span>，也就相当于 196 个 维度为 512 的 local regional 向量</li>
</ul></li>
<li><strong>Visual feature embedding</strong>
<ul>
<li>使用 tanh 激活函数的 linear layer</li>
<li>用于将 local regional 向量投射到 question 向量使用的文字特征空间</li>
</ul></li>
<li><strong>Input fusion layer</strong>
<ul>
<li>和针对 Text QA 的 input module 里的 Input fusion layer 一样</li>
</ul></li>
</ul>
<h4 id="事件记忆模块">事件记忆模块</h4>
<p><strong>模块示例图</strong> <img src="/memory-networks-summary/1526605717780.png" alt="Alt text"></p>
<p>attention gate <span class="math inline">\(g_i^t\)</span> 的计算如下</p>
<p><span class="math display">\[\begin{align*}
&amp;z_i^t=[\overleftrightarrow{f_i}\circ q;\overleftrightarrow{f_i}\circ m^{t-1};|\overleftrightarrow{f_i}-q|;|\overleftrightarrow{f_i}-m^{t-1}|] \\&amp;&amp;&amp;&amp;\\
&amp;Z_i^t=W^{(2)}\tanh (W^{(1)}z_i^t+b^{(1)})+b^{(2)} \\&amp;&amp;&amp;&amp;\\
&amp;g_i^t=Softmax(Z^t)=\frac{\exp(Z_i^t)}{\sum_{k=1}^{M_i}\exp(Z_k^t)}
\end{align*}\]</span></p>
<p>其中， <span class="math inline">\(\overleftrightarrow{f_i}\)</span> 是第 <span class="math inline">\(i\)</span> 个 fact，<span class="math inline">\(m^{t-1}\)</span> 是前一个 memory，<span class="math inline">\(q\)</span> 是原始问题，<span class="math inline">\(\circ\)</span> 是 element-wise product，<span class="math inline">\(|\cdot|\)</span> 是 element-wise 绝对值</p>
<p><img src="/memory-networks-summary/1526605726799.png" alt="Alt text"> (a) 为传统 GRU 模块，(b) 为基于 attention 的 GRU 模块</p>
<p>事件记忆模块主要分为两部分：<strong>Attention Mechanism</strong> 和 <strong>Episode Memory Update</strong></p>
<ul>
<li><strong>Attention 机制有两种选择</strong>
<ul>
<li><strong>Soft Attention</strong>
<ul>
<li>直接求加权和，contextual 向量 <span class="math inline">\(c^t=\sum_{i=1}^Ng_i^t\overleftrightarrow{f_i}\)</span></li>
<li>两个优点：便于计算；如果 softmax 激活是 spiky 的，它可以通过仅选择一个 fact 作为 contextual 向量来保持可微分，接近于 hard attention 函数(?)</li>
<li>缺点：求和过程丢失了位置和排序的信息</li>
</ul></li>
<li><strong>基于 Attention 的 GRU</strong>
<ul>
<li>参考上图，把 GRU 单元里的 <span class="math inline">\(u_i\)</span> 换成了 <span class="math inline">\(g_i^t\)</span></li>
<li><span class="math inline">\(h_i=g_i^t\circ\tilde h_i+(1-g_i^t)\circ h_{i-1}\)</span></li>
<li><span class="math inline">\(g_i^t\)</span> 是一个使用 Softmax 计算的标量，而 <span class="math inline">\(u_i\)</span> 是一个使用 Sigmoid 计算的 <span class="math inline">\(n_H\)</span> 维向量</li>
</ul></li>
</ul></li>
<li><strong>Episode Memory 更新</strong>
<ul>
<li>不再像 DMN 里一样用 GRU，而是用了 ReLU layer 来更新 memory</li>
<li><span class="math inline">\(m^t=ReLU(W^t[m^{t-1};c^t;q]+b)\)</span></li>
</ul></li>
</ul>
<h2 id="总结">总结</h2>
<ol type="1">
<li>重新读了一遍论文，修改了不少原始笔记里的理解错误，希望每读一遍都能理解的更深入一点</li>
</ol>

      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/MemNNs/" rel="tag"># MemNNs</a>
          
            <a href="/tags/MemN2N/" rel="tag"># MemN2N</a>
          
            <a href="/tags/DMN/" rel="tag"># DMN</a>
          
            <a href="/tags/DMN/" rel="tag"># DMN+</a>
          
            <a href="/tags/KV-MemNNs/" rel="tag"># KV-MemNNs</a>
          
            <a href="/tags/学习笔记/" rel="tag"># 学习笔记</a>
          
            <a href="/tags/未完成/" rel="tag"># 未完成</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/lightgbm-summary/" rel="next" title="学习笔记——LightGBM（Light Gradient Boosting Machine）">
                <i class="fa fa-chevron-left"></i> 学习笔记——LightGBM（Light Gradient Boosting Machine）
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/Keras-Embedding-Initializer/" rel="prev" title="关于 Keras 的 Embedding Initializer">
                关于 Keras 的 Embedding Initializer <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          

  
    <div class="comments" id="comments">
    </div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">rocuku</p>
              <p class="site-description motion-element" itemprop="description">I know nothing except the fact of my ignorance</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">33</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">5</span>
                    <span class="site-state-item-name">categories</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">34</span>
                    <span class="site-state-item-name">tags</span>
                  </a>
                </div>
              
            </nav>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  <a href="https://github.com/rocuku" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i>GitHub</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:rocuku@gmail.com" target="_blank" title="E-Mail"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  
                </span>
              
            </div>
          

          
          

          
          

          
            
          
          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#memory-networks"><span class="nav-number">1.</span> <span class="nav-text">Memory Networks</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#基本框架"><span class="nav-number">1.1.</span> <span class="nav-text">基本框架</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#memnns"><span class="nav-number">1.2.</span> <span class="nav-text">MemNNs</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#基本模型"><span class="nav-number">1.2.1.</span> <span class="nav-text">基本模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#训练"><span class="nav-number">1.2.2.</span> <span class="nav-text">训练</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#一些扩展"><span class="nav-number">1.2.3.</span> <span class="nav-text">一些扩展</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#针对-large-scale-simple-qa-的-memnns"><span class="nav-number">1.3.</span> <span class="nav-text">针对 Large-scale Simple QA 的 MemNNs</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#基本模型-1"><span class="nav-number">1.3.1.</span> <span class="nav-text">基本模型</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#memn2n"><span class="nav-number">2.</span> <span class="nav-text">MemN2N</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#基本模型-2"><span class="nav-number">2.1.</span> <span class="nav-text">基本模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#单层模型"><span class="nav-number">2.1.1.</span> <span class="nav-text">单层模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#多层模型"><span class="nav-number">2.1.2.</span> <span class="nav-text">多层模型</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#kv-memnns"><span class="nav-number">3.</span> <span class="nav-text">KV-MemNNs</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#基本模型-3"><span class="nav-number">3.1.</span> <span class="nav-text">基本模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#key-value-实现方式"><span class="nav-number">3.2.</span> <span class="nav-text">Key-Value 实现方式</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#dmn"><span class="nav-number">4.</span> <span class="nav-text">DMN</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#模型"><span class="nav-number">4.1.</span> <span class="nav-text">模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#input-module"><span class="nav-number">4.1.1.</span> <span class="nav-text">Input module</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#question-module"><span class="nav-number">4.1.2.</span> <span class="nav-text">Question Module</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#episodic-memory-module"><span class="nav-number">4.1.3.</span> <span class="nav-text">Episodic Memory Module</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#answer-module"><span class="nav-number">4.1.4.</span> <span class="nav-text">Answer Module</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#dmn-1"><span class="nav-number">5.</span> <span class="nav-text">DMN+</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#模型-1"><span class="nav-number">5.1.</span> <span class="nav-text">模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#针对-text-qa-的-input-module"><span class="nav-number">5.1.1.</span> <span class="nav-text">针对 Text QA 的 input module</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#针对-vqa-的-input-module"><span class="nav-number">5.1.2.</span> <span class="nav-text">针对 VQA 的 input module</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#事件记忆模块"><span class="nav-number">5.1.3.</span> <span class="nav-text">事件记忆模块</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结"><span class="nav-number">6.</span> <span class="nav-text">总结</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div>「I know nothing except the fact of my ignorance.」</div>
<div class="copyright">&copy; 2016 &mdash; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">rocuku</span>

  

  
</div>




  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/theme-next/hexo-theme-next">NexT.Mist</a> v6.0.6</div>




        








  <div style="display: none;">
    <script src="https://s22.cnzz.com/z_stat.php?id=1273679041&web_id=1273679041" language="JavaScript"></script>
  </div>



        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.0.6"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.0.6"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=6.0.6"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=6.0.6"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.0.6"></script>



  



	





  





  








  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  
  
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(function (item) {
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'dr0tt1EJca5IeOAwrgmemg4h-gzGzoHsz',
        appKey: 'qFvYTrsP9GkeaYJtvIzsbCMR',
        placeholder: 'Just go go',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  





  

  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    
  


  
  

  

  

  

  

</body>
</html>
